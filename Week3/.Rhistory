geom_smooth()
#Convert cyl to factor
mtcars$cyl <- as.factor(mtcars$cyl)
#Example from base R
plot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)
abline(lm(mpg ~ wt, data = mtcars), lty = 2)
lapply(mtcars$cyl, function(x) {
abline(lm(mpg ~ wt, mtcars, subset = (cyl == x)), col = x)
})
library(tidyr)
iris.tidy <- iris %>%
gather(key, Value, -'Species') %>%
separate(key, c("Part", "Measure"), "\\.")
iris.tidy
ggplot(iris.tidy, aes(x = Species, y = Value, col = Part)) +
geom_jitter() +
facet_grid(. ~ Measure)
ggplot(diamonds, aes(x = carat, y = price, color = clarity)) +
geom_smooth()
library(ggplot2)
library(tidyr)
#iris.tidy
iris.tidy <- iris %>%
gather(key, Value, -'Species') %>%
separate(key, c("Part", "Measure"), "\\.")
head(iris.tidy)
ggplot(iris.tidy, aes(x = Species, y = Value, col = Part)) +
geom_jitter() +
facet_grid(. ~ Measure)
#iris.wide
#iris$Flower <- 1:nrow(iris)
iris.wide <- iris %>%
gather(key, value, -Species) %>%
separate(key, c('Part','Measure'), "\\.") %>%
spread(Measure, value)
setwd("D:/GitHub/NTU-CS-X/Week3")
docs <- readLines("./Data/EDA/conan1.txt")
docs <- gsub("\\[(0-9)+\\]", "", docs)# 去除(集數)
docs.corpus <- Corpus(VectorSource(docs))# 把 docs 轉成 corpus
docs.seg <- tm_map(docs.corpus, segmentCN)# 斷詞
docs.tdm <- TermDocumentMatrix(docs.seg, control = list())# 斷詞結果轉換成 term-document matrix
inspect(docs.tdm)
library(tm)
library(tmcn)
library(Matrix)
library(wordcloud)
install.packages('tm')
install.packages('tmcn')
install.packages('Matrix')
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages('wordcloud')
docs <- readLines("./Data/EDA/conan1.txt")
docs <- gsub("\\[(0-9)+\\]", "", docs)# 去除(集數)
docs.corpus <- Corpus(VectorSource(docs))# 把 docs 轉成 corpus
docs.seg <- tm_map(docs.corpus, segmentCN)# 斷詞
docs.tdm <- TermDocumentMatrix(docs.seg, control = list())# 斷詞結果轉換成 term-document matrix
inspect(docs.tdm)
library(tm)
library(tmcn)
library(Matrix)
docs <- readLines("./Data/EDA/conan1.txt")
docs <- gsub("\\[(0-9)+\\]", "", docs)# 去除(集數)
docs.corpus <- Corpus(VectorSource(docs))# 把 docs 轉成 corpus
docs.seg <- tm_map(docs.corpus, segmentCN)# 斷詞
docs.tdm <- TermDocumentMatrix(docs.seg, control = list())# 斷詞結果轉換成 term-document matrix
inspect(docs.tdm)
docs <- readLines("./Data/EDA/conan1.txt")
docs <- gsub("\\[(0-9)+\\]", "", docs)# 去除(集數)
docs.corpus <- Corpus(VectorSource(docs))# 把 docs 轉成 corpus
docs.seg <- tm_map(docs.corpus, segmentCN)# 斷詞
docs.tdm <- TermDocumentMatrix(docs.seg, control = list())# 斷詞結果轉換成 term-document matrix
inspect(docs.tdm)
install.packages('jiebaR')
docs <- readLines("./Data/EDA/conan1.txt")
docs <- gsub("\\[(0-9)+\\]", "", docs)# 去除(集數)
docs.corpus <- Corpus(VectorSource(docs))# 把 docs 轉成 corpus
docs.seg <- tm_map(docs.corpus, segmentCN)# 斷詞
docs.tdm <- TermDocumentMatrix(docs.seg, control = list())# 斷詞結果轉換成 term-document matrix
inspect(docs.tdm)
docs.tf <- apply(as.matrix(docs.tdm), 2, function(doc) {doc / sum(doc)})# 計算每個詞的 term frequency
idf.function <- function(word_doc) { log2( (length(word_doc)+1) / nnzero(word_doc) ) }# 定義計算 idf 的 function
docs.idf <- apply(docs.tdm, 1, idf.function)# 計算 idf
docs.tfidf <- docs.tf * docs.idf# tfidf = tf * idf
head(docs.tfidf)# 看結果
View(docs.tfidf)
cos <- function(x, y){
return (x %*% y / sqrt(x %*% x * y %*% y))[1, 1]
}
# compare with first doc
docs.cos.sim <- apply(docs.tfidf, 2, cos, y = docs.tfidf[, 1])
docs.cos.sim
f <- sort(rowSums(docs.tfidf), decreasing = T)
docs.df <- data.frame(
word = names(f),
freq = f
)
wordcloud(docs.df$word, docs.df$freq, scale=c(5,0.1), colors=brewer.pal(8, "Dark2"))
library(wordcloud)
f <- sort(rowSums(docs.tfidf), decreasing = T)
docs.df <- data.frame(
word = names(f),
freq = f
)
wordcloud(docs.df$word, docs.df$freq, scale=c(5,0.1), colors=brewer.pal(8, "Dark2"))
View(docs.tfidf)
query.tfidf <- function(q){
q.position <- which(rownames(docs.tfidf) %in% q)
q.tfidf <- docs.tfidf[q.position, ]
return (q.tfidf)
}
query.tfidf(c("柯南", "小蘭", "小五郎"))
query.tfidf(c("殺人", "小蘭", "小五郎"))
docs <- readLines("./Data/EDA/conan2.txt")
docs <- gsub("\\[[0-9]+\\]", "", docs)# 去除[注]
docs.corpus <- Corpus(VectorSource(docs))# 把 docs 轉成 corpus
docs.seg <- tm_map(docs.corpus, segmentCN)# 斷詞
docs.tdm <- TermDocumentMatrix(docs.seg, control = list())# 斷詞結果轉換成 term-document matrix
inspect(docs.tdm)
docs.tf <- apply(as.matrix(docs.tdm), 2, function(doc) {doc / sum(doc)})# 計算每個詞的 term frequency
idf.function <- function(word_doc) { log2( (length(word_doc)+1) / nnzero(word_doc) ) }# 定義計算 idf 的 function
docs.idf <- apply(docs.tdm, 1, idf.function)# 計算 idf
docs.tfidf <- docs.tf * docs.idf# tfidf = tf * idf
head(docs.tfidf)# 看結果
query.tfidf <- function(q){
q.position <- which(rownames(docs.tfidf) %in% q)
q.tfidf <- docs.tfidf[q.position, ]
return (q.tfidf)
}
query.tfidf(c("殺人", "小蘭", "小五郎"))
query.tfidf <- function(q){
q.position <- which(rownames(docs.tfidf) %in% q)
q.tfidf <- docs.tfidf[q.position, ]
return (q.tfidf)
}
query.tfidf(c("柯南", "小蘭", "小五郎"))
cos <- function(x, y){
return (x %*% y / sqrt(x %*% x * y %*% y))[1, 1]
}
# compare with first doc
docs.cos.sim <- apply(docs.tfidf, 2, cos, y = docs.tfidf[, 1])
docs.cos.sim
f <- sort(rowSums(docs.tfidf), decreasing = T)
docs.df <- data.frame(
word = names(f),
freq = f
)
wordcloud(docs.df$word, docs.df$freq, scale=c(5,0.1), colors=brewer.pal(8, "Dark2"))
wordcloud(docs.df$word, docs.df$freq, scale=c(5,0.1), colors=brewer.pal(8, "Dark2"))
View(docs.tfidf)
f <- sort(rowSums(docs.tfidf), decreasing = T)
docs.df <- data.frame(
word = names(f),
freq = f
)
f <- sort(rowSums(docs.tfidf), decreasing = T)
b
rowSums(docs.tfidf)
setwd("D:/GitHub/NTU-CS-X/Week3")
d.corpus <- Corpus( DirSource("./Data") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
library(knitr)
kable(head(TDM))
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
install.packages('httr')
install.packages('RCurl')
install.packages('XML')
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
# 進行斷詞
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
library(knitr)
kable(head(TDM))
kable(tail(TDM))
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum)
library(Matrix)
idfCal <- function(word_doc)
{
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(TDM[,2:(n+1)]), 1, idfCal)
doc.tfidf <- TDM
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
doc.tfidf[,2:(n+1)] <- (doc.tfidf[,2:(n+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(n+1)])
delID = which(stopLine == 0)
kable(head(doc.tfidf[delID,1]))
kable(tail(doc.tfidf[delID,1]))
TDM = TDM[-delID,]
doc.tfidf = doc.tfidf[-delID,]
TopWords = data.frame()
for( id in c(1:n) )
{
dayMax = order(doc.tfidf[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(doc.tfidf[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
kable(TopWords)
install.packages('varhandle')
kable(head(AllTop))
TopNo = 5
tempGraph = data.frame()
for( t in c(1:TopNo) )
{
word = matrix( rep(c(as.matrix(AllTop$Var1[t])), each = n), nrow = n )
temp = cbind( colnames(doc.tfidf)[2:(n+1)], t(TDM[which(TDM$d == AllTop$Var1[t]), 2:(n+1)]), word )
colnames(temp) = c("hour", "freq", "words")
tempGraph = rbind(tempGraph, temp)
names(tempGraph) = c("hour", "freq", "words")
}
library(ggplot2)
library(varhandle)
tempGraph$freq = unfactor(tempGraph$freq)
ggplot(tempGraph, aes(hour, freq)) +
geom_point(aes(color = words, shape = words), size = 5) +
geom_line(aes(group = words, linetype = words))
doc.tfidf = doc.tfidf[-delID,]
TDM$d = as.character(TDM$d)
AllTop = as.data.frame( table(as.matrix(TopWords)) )
AllTop = AllTop[order(AllTop$Freq, decreasing = TRUE),]
kable(head(AllTop))
TopNo = 5
tempGraph = data.frame()
for( t in c(1:TopNo) )
{
word = matrix( rep(c(as.matrix(AllTop$Var1[t])), each = n), nrow = n )
temp = cbind( colnames(doc.tfidf)[2:(n+1)], t(TDM[which(TDM$d == AllTop$Var1[t]), 2:(n+1)]), word )
colnames(temp) = c("hour", "freq", "words")
tempGraph = rbind(tempGraph, temp)
names(tempGraph) = c("hour", "freq", "words")
}
library(ggplot2)
library(varhandle)
tempGraph$freq = unfactor(tempGraph$freq)
ggplot(tempGraph, aes(hour, freq)) +
geom_point(aes(color = words, shape = words), size = 5) +
geom_line(aes(group = words, linetype = words))
doc.tfidf = doc.tfidf[-delID,]
filenames = as.array(paste0("./DATA/",colnames(doc.tfidf)[2:(n+1)],".txt"))
sizeResult = apply(filenames, 1, file.size) / 1024
showSize = data.frame(colnames(doc.tfidf)[2:(n+1)], sizeResult)
names(showSize) = c("hour", "size_KB")
ggplot(showSize, aes(x = hour, y = size_KB)) + geom_bar(stat="identity")
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
# PTT 網路爬蟲抓出所有文章內文所對應的網址
from <- 2938
to   <- 2943
prefix = "https://www.ptt.cc/bbs/PC_Shopping/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
}
data <- unlist(data)
head(data)
# 利用所有文章的網址去抓所有文章內文, 並解析出文章的內容並依照 hour 合併儲存。
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
name <- paste0('./Data/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
# 文本資料清洗
d.corpus <- Corpus( DirSource("./Data") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
library(knitr)
kable(head(TDM))
kable(tail(TDM))
# 轉成TF-IDF
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum)
library(Matrix)
idfCal <- function(word_doc)
{
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(TDM[,2:(n+1)]), 1, idfCal)
doc.tfidf <- TDM
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
doc.tfidf[,2:(n+1)] <- (doc.tfidf[,2:(n+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(n+1)])
delID = which(stopLine == 0)
kable(head(doc.tfidf[delID,1]))
kable(tail(doc.tfidf[delID,1]))
TDM = TDM[-delID,]
# 取得關鍵字
TopWords = data.frame()
for( id in c(1:n) )
{
dayMax = order(doc.tfidf[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(doc.tfidf[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
kable(TopWords)
# TDM視覺化
TDM$d = as.character(TDM$d)
AllTop = as.data.frame( table(as.matrix(TopWords)) )
AllTop = AllTop[order(AllTop$Freq, decreasing = TRUE),]
kable(head(AllTop))
TopNo = 5
tempGraph = data.frame()
for( t in c(1:TopNo) )
{
word = matrix( rep(c(as.matrix(AllTop$Var1[t])), each = n), nrow = n )
temp = cbind( colnames(doc.tfidf)[2:(n+1)], t(TDM[which(TDM$d == AllTop$Var1[t]), 2:(n+1)]), word )
colnames(temp) = c("hour", "freq", "words")
tempGraph = rbind(tempGraph, temp)
names(tempGraph) = c("hour", "freq", "words")
}
library(ggplot2)
library(varhandle)
tempGraph$freq = unfactor(tempGraph$freq)
ggplot(tempGraph, aes(hour, freq)) +
geom_point(aes(color = words, shape = words), size = 5) +
geom_line(aes(group = words, linetype = words))
doc.tfidf = doc.tfidf[-delID,]
# 發文時間與發文量
filenames = as.array(paste0("./DATA/",colnames(doc.tfidf)[2:(n+1)],".txt"))
sizeResult = apply(filenames, 1, file.size) / 1024
showSize = data.frame(colnames(doc.tfidf)[2:(n+1)], sizeResult)
names(showSize) = c("hour", "size_KB")
ggplot(showSize, aes(x = hour, y = size_KB)) + geom_bar(stat="identity")
View(doc.tfidf)
# PCA
docs.pca <- prcomp(docs.tfidf, scale = T)
doc.tfidf <- docs.tfidf
doc.tfidf <- docs.tfidf
docs.tfidf <- doc.tfidf
docs.pca <- prcomp(docs.tfidf, scale = T)
docs.corpus <- Corpus(DirSource("./Data"))
docs.seg <- tm_map(docs.corpus, segmentCN)
docs.tdm <- TermDocumentMatrix(docs.seg)
docs.tf <- apply(as.matrix(docs.tdm), 2, function(word) { word/sum(word) })
idf <- function(doc) {
return ( log2( length(doc)+1 / nnzero(doc)) )
}
docs.idf <- apply(as.matrix(docs.tdm), 1, idf)
docs.tfidf <- docs.tf * docs.idf
docs.pca <- prcomp(docs.tfidf, scale = T)
fviz_eig(docs.pca)
fviz_pca_ind(docs.pca, geom.ind = c("point"), col.ind = "cos2")
fviz_pca_var(docs.pca, col.var = "contrib")
fviz_pca_biplot(docs.pca, geom.ind = "point")
# PCA Results
docs.eig <- get_eig(docs.pca)
docs.var <- get_pca_var(docs.pca)
docs.ind <- get_pca_ind(docs.pca)
# Kmeans
# Choosing K
ind.coord2 <- docs.ind$coord[, 1:2]
wss <- c()
for (i in 1:10) { wss[i] <- kmeans(ind.coord2, i)$tot.withinss }
plot(wss, type = "b")
# Clustering
km <- kmeans(ind.coord2, 3)
plot(ind.coord2, col = km$cluster)
points(km$centers, col = 1:3, pch = 8, cex = 2)
install.packages('factoextra')
docs.pca <- prcomp(docs.tfidf, scale = T)
# Drawing
fviz_eig(docs.pca)
fviz_pca_ind(docs.pca, geom.ind = c("point"), col.ind = "cos2")
fviz_pca_var(docs.pca, col.var = "contrib")
fviz_pca_biplot(docs.pca, geom.ind = "point")
# PCA Results
docs.eig <- get_eig(docs.pca)
docs.var <- get_pca_var(docs.pca)
docs.ind <- get_pca_ind(docs.pca)
# Kmeans
# Choosing K
ind.coord2 <- docs.ind$coord[, 1:2]
wss <- c()
for (i in 1:10) { wss[i] <- kmeans(ind.coord2, i)$tot.withinss }
plot(wss, type = "b")
# Clustering
km <- kmeans(ind.coord2, 3)
plot(ind.coord2, col = km$cluster)
points(km$centers, col = 1:3, pch = 8, cex = 2)
library(factoextra)
# PCA
docs.pca <- prcomp(docs.tfidf, scale = T)
# Drawing
fviz_eig(docs.pca)
fviz_pca_ind(docs.pca, geom.ind = c("point"), col.ind = "cos2")
fviz_pca_var(docs.pca, col.var = "contrib")
fviz_pca_biplot(docs.pca, geom.ind = "point")
# PCA Results
docs.eig <- get_eig(docs.pca)
docs.var <- get_pca_var(docs.pca)
docs.ind <- get_pca_ind(docs.pca)
# Kmeans
# Choosing K
ind.coord2 <- docs.ind$coord[, 1:2]
wss <- c()
for (i in 1:10) { wss[i] <- kmeans(ind.coord2, i)$tot.withinss }
plot(wss, type = "b")
# Clustering
km <- kmeans(ind.coord2, 3)
plot(ind.coord2, col = km$cluster)
points(km$centers, col = 1:3, pch = 8, cex = 2)
