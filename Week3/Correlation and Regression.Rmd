---
title: "Correlation and Regression"
author: "Bourbon0212"
date: "2018年7月23日"
output: html_document
---

* [Visualizing Bivariate Relationships](#1)   
* [Correlation](#2)   
    + [What is correlation?](#2.1)    
    + [Anscombe](#2.2)    
* [Simple Linear Regression](#3)    
    + [Method od Linear Regression](#3.1)     
    + [Automatic - Smooth](#3.2)    
    + [Manual - Abline](#3.3)   
    + [Regression to Mean](#3.4)    
* [Coefficients in Regression Models](#4)     
    + [Linear Model - `lm()`](#4.1)
    + [Use of Regression Model](#4.2)   
        + [Prediction: Called out-of-sample](#4.2.1)    
        + [Adding Regression Line to Plot Manually](#4.2.2)
* [Model Fit](#5)   
    + [Assess Model Fit](#5.1)    
        + [SSE](#5.1.1)     
        + [RMSE](#5.1.2)    
    + [Compare Model Fit](#5.2)     
        + [Null(Average) Model](#5.2.1)   
        + [R^2](#5.2.2)     
    + [Unsual Points](#5.3)     
        + [Leverage](#5.3.1)      
        + [Influence](#5.3.2)   
        + [Removing Outliers](#5.3.3)   

<h2 id = '1'></h2>
# **Visualizing Bivariate Relationships**   

In this chapter, we are going to talk about **two (continuous) numerical variables**.     

* Response Variable, also known as `y` which is dependent.    
* Explanatory Variable, also known as `x` which is independent and a predictor.   
    + Something you think might be related to the response.   
* Characterizing bivariate relationships.   
    + Form: linear, quadratic, non-linear etc.    
    + Direction: positive, negative.    
    + Strength: how much scatter/noise?   
    + Outliers.     

<br>

Through out the chapter, we'll do some practices on several datasets listed below inside `openintro` package.     

* `ncbirths`: A random sample of 1,000 cases taken from a larger dataset collected in 2004 describing the birth of a single child born in North Carolina, along with various characteristics of the child.    
* `mammals`: Contains information about 39 species of mammals, including their body weight, brain weight, gestation time, etc.       
* `mlbBat10`: Contains batting statistics for 1,199 Major League Baseball players during the 2010 season.     
* `bdims`: Contains body girth and skeletal diameter measurements for 507 physically active individuals.      
* `smoking`: Contains information on the smoking habits of 1,691 citizens of the United Kingdom.      

<br>

#### **Example1 : ncbirths**    

Using the `ncbirths` dataset, make a scatterplot using ggplot() to illustrate how the birth weight of these babies varies according to the number of weeks of gestation.
```{r message = FALSE, warning = F}
library(ggplot2)
library(openintro)
library(dplyr)
# Use geom_plot() to make a scatterplot.
ggplot(ncbirths, aes(x = weeks, y = weight)) +
  geom_point()

# Use cut() to break a continuous variable to a discrete one.
ggplot(data = ncbirths, 
       aes(x = cut(weeks, breaks = 5), y = weight)) + 
  geom_boxplot()
```

In the scatterplot, it seems that there is a positive relationship between `weeks` & `weight`.   
However, after using `cut()` discretizing continuous variables and drawing boxplot, the relationship no longer seems linear.    

<br>

#### **Example2: Mammals**

Using the `mammals` dataset, create a scatterplot illustrating how the brain weight of a mammal varies as a function of its body weight.    
```{r}
# Scatterplot with original scale.
ggplot(mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point()

# Scatterplot with coord_trans().
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() + 
  coord_trans(x = "log10", y = "log10")

# Scatterplot with scale_x_log10() and scale_y_log10().
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() +
  scale_x_log10() + scale_y_log10()
```

Because of extreme values in this dataset, it's hard to see the relationship between `BodyWt` & `BrainWt`.    
We can put these variables on a log scale and noticed that the relation is much clearer now.    

* `coord_trans()`: [Coord_() function](https://bourbon0212.github.io/NTU-CS-X/Week3/Data_Visualization_with_ggplot2__Part_2_.html#2) to put it on a log10 scale.         
* `scale_x_continuous()` [Scale_() function](https://bourbon0212.github.io/NTU-CS-X/Week2/Data_Visualization_with_ggplot2__Part_1_.html#3.3) can do the same thing with different lables on axises.        

<br>

#### **Example3: mlbBat10**   

Using the `mlbBat10` dataset, create a scatterplot illustrating how the slugging percentage (SLG) of a player varies as a function of his on-base percentage (OBP). 
```{r}
# Scatterplot with outliers.
ggplot(mlbBat10, aes(x = OBP, y = SLG)) +
  geom_point()

# After removing those outliers.
mlbBat10 %>%
  filter(AB >= 200) %>%
  ggplot(aes(x = OBP, y = SLG)) +
  geom_point()
```

In the first scatterplot, most of the points were clustered in the lower left corner of the plot, making it difficult to see the general pattern of the majority of the data. This difficulty was caused by a few **outlying** players whose on-base percentages (OBPs) were exceptionally high.   

After removing outliers, we can finally see the general pattern in the dataset, we'll discuss the influence of outliers later.

<br>

<h2 id = '2'></h2>
# **Correlation**

<h3 id = '2.1'></h3>
## What is correlation?    

* Correlation coefficient between `-1` and `1`.   
* Sign -> Direction.    
* Magnitude -> Strength.    
* `cor(x, y)`  Compute the Pearson product-moment correlation between variables, `x` and `y`, and the order doesn't matter.     
    + `cor()` will return `NA` as a default if there is missing data.     
    + Use `use = "pairwise.complete.obs"` when there is missing values in the dataset and `cor()` will ignore those values.     

<br>

#### **Example1: ncbirths**
```{r}
ncbirths %>%
  summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))
```

#### **Example2: mammals**
```{r}
# Correlation among mammals, with and without log
mammals %>%
  summarize(N = n(), 
            r = cor(BodyWt, BrainWt), 
            r_log = cor(log(BodyWt), log(BrainWt)))
```

<br>

<h3 id = '2.2'></h3>
## Anscombe

In 1973, Francis Anscombe famously created four datasets with remarkably similar numerical properties, but obviously different graphic relationships.   
For how to tidy up `anscombe` to `Anscombe` using here, look for it in the **[RMD](https://github.com/Bourbon0212/NTU-CS-X/blob/master/Week3/Correlation%20and%20Regression.Rmd#2.2)** of this course.    

```{r echo = FALSE}
# Tidy up anscombe dataset
library(tidyr)
library(stringr)
Anscombe <-anscombe %>%
  mutate(id = c(1:11)) %>%
  gather(key_x, x, x1:x4) %>%
  gather(key_y, y, y1:y4) %>%
  arrange(id)
Anscombe$key_x <- str_replace(Anscombe$key_x,'x','')
Anscombe$key_y <- str_replace(Anscombe$key_y,'y','')
Anscombe <- filter(Anscombe, key_x == key_y) %>%
  select('id', 'x', 'y')
Anscombe$set <- rep(c(1:4))
```
```{r}
# Plot of Anscombe and wrapping according to set.
ggplot(data = Anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ set)
```
```{r}
Anscombe %>%
  group_by(set) %>%
  summarize(N = n(), mean(x), sd(x), mean(y), sd(y), cor(x, y))
```

Although their plots look totally different, all of the measures are identical (ignoring rounding error) across the four different sets.    

<br>

<h2 id = '3'></h2>
# **Simple Linear Regression**    


<h3 id = '3.1'></h3>
## Method of Linear Regression

In this section, we want to find out **the 'best fit' line** by a method called **Linear Regression**.    
The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a "best fit" line that cuts through the data in a way that **minimizes** the distance between the line and the data points.        


#### **Generic Statistical Model**
> ***`response = f(explanatory) + noise`***

#### **Generic Linear Model**
> ***`response = intercept + (slope * explanatory) + noise`***    

#### **Least Squares**
In this section we'll use **Least Squares** as a method of linear regression.     

* Easy, deterministic, unique solutions.    
* Residuals sum to zero.   
    + Residuals = Observation(`y`) - Expected(`y-hat`)
    + Y-hat is expected value given corresponding X.    
    + Beta-hats are estimates of true, unknown betas.   
    + Residuals are estimates of true, unknown epsilons.    
        + 'Error' is misleading, it should be called 'Noise'.   
    + Reference: **[Chi-Squared Test](https://bourbon0212.github.io/NTU-CS-X/Week3/Data_Visualization_with_ggplot2__Part_2_.html#5.2)**   
* Line must pass through `(mean(x), mean(y))`

<br>

<h3 id = '3.2'></h3>
## Automatic - Smooth   

Fortunately, we don't need to calculate those values on our own.    
In `ggplot2` we can simply call out `geom_smooth(method = 'lm')` to find the 'best fit' line in a blink.    

* Reference: **[Statistics - Smooth](https://bourbon0212.github.io/NTU-CS-X/Week3/Data_Visualization_with_ggplot2__Part_2_.html#1.2)**
* `method = 'lm'` means using a linear model.

<br>

#### **Example: bdims**   

```{r}
# Scatterplot with regression line using geom_smooth()
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = FALSE)
```

<br>

<h3 id = '3.3'></h3>
## Manual - Abline
Besides using `geom_smooth()` to get the 'best fit' line, we can set it up manually.    
If we choose to calculate on ourselves, we need to know how a regression line comes out first.      

#### **Linear Model**    
> ***`Y = m ⋅ X + k`***     
> ***`m = r(X,Y) ⋅ SD(Y)/SD(X)`***     

* Two facts to compute: the slope(`m`) and intercept(`k`) of a simple linear regression model from some basic summary statistics.   
* `r(x,y)` represents the correlation of `X` & `Y`.   
    + `cor(x,y)` to compute the correlation coefficient.    
* `SD(X)` & `SD(Y)` represents the standard deviation of `X` & `Y`.   
    + `sd()` to compute the standard deviation.  
* `mean(X)` & `mean(Y)` is **always** on the least squares regression line.
* Use `geom_abline(slope = *, intersect = *)` to add a line on the plot manually.   

<br>

#### **Example: bdims**
```{r}
# Calculate summary statistics
bdims_summary <- summarize(bdims, N = n(), r = cor(hgt, wgt),
                           mean_hgt = mean(hgt), sd_hgt = sd(hgt),
                           mean_wgt = mean(wgt), sd_wgt = sd(wgt))
bdims_summary <- mutate(bdims_summary, slope = r*sd_wgt/sd_hgt, 
                        intercept = mean_wgt - r*sd_wgt/sd_hgt*mean_hgt)
bdims_summary
# Scatterplot with regression line using geom_abline()
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_abline(slope = as.numeric(bdims_summary$slope), intercept = as.numeric(bdims_summary$intercept))
```

<br>

<h3 id = '3.4'></h3>
## Regression to mean

Regression to the mean is a concept attributed to Sir Francis Galton.     
The basic idea is that extreme random observations will tend to be less extreme upon a second trial.    

<br>

#### **Example1: Galton_men & Galton_women**
```{r}
library(HistData)
# Height of children vs. parent
ggplot(data = Galton, aes(x = parent, y = child)) +
  geom_jitter(alpha = 0.3) + 
  # Add a diagonal line which slopes = 1
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = 'lm', se = FALSE)
```

Because the slope of the regression line is smaller than 1 (the slope of the diagonal line), we can verify Sir Francis Galton's regression to the mean concept!

<br>

#### **Example2: The New York Times in 2015**   

> *"Regression to the mean is so powerful that once-in-a-generation talent basically never sires once-in-a-generation talent.It explains why Michael Jordan’s sons were middling college basketball players and Jakob Dylan wrote two good songs. It is why there are no American parent-child pairs among Hall of Fame players in any major professional sports league."*

The author is arguing that because of regression to the mean, an outstanding basketball player is likely to have sons that are good at basketball, but **not** as good as him.

<br>
<h2 id = '4'></h2>
# **Coefficients in Regression Models**   

<h3 id = '4.1'></h3>
## Linear Model - `lm()`

While the `geom_smooth(method = "lm")` function is useful for drawing linear models on a scatterplot, it doesn't actually return the characteristics of the model. On the other hand, function `lm()` does!   

* `lm(Response(y) ~ Explanatory(x), data = *)`    
    + `(y ~ x)`: A `formula` that specifies the model.    
    + `data`: Argument for the data frame that contains the data you want to use to fit the model.    
<br>
* Return a model object having class `"lm"`.    
<br>
* Contains lots of information about your regression model.   
    + The data used to fit the model.   
    + The specification of the model.   
    + The fitted values and residuals, etc.   
<br>    
* `coef(model)`: Display the coefficients of the model.    
<br>
* `summary(model)`: Display the full regression output of the model.   
<br>
* Recall that `Residuals = Observations - Expected`   
    + `Obsrvation` are the values in the dataset.   
    + `Expected` are fitted values fits this model.   
    + `fitted.values(model)`: Returns the expected(fitted) values of the model.   
    + `residuals(model)`: Returns the residuals of the model.     
<br>
<h4 id = 'augment()'></h4>
* Tidy linear model:    
    + `augment(model)`: Returns a dataframe containing the data on which the model was fit and several quantities specific to the regression model.   
    + `augment(model)` is from the `broom` package.   

<br>

#### **Example: bdims**
```{r}
# Linear model for weight as a function of height
mod <- lm(wgt ~ hgt, data = bdims)
mod
```

Recall that a general linear model lookes like this:    

> ***`Y = m ⋅ X + k`***     

* `(Intercept)`: refers to the intercept(`k`) of the model.   
* `hgt`: refers to the slope(`m`) of the model in this example.   

```{r}
# Show the coefficients
coef(mod)

# Show the full output
summary(mod)

# Mean of weights equal to mean of fitted values?
mean(bdims$wgt) == mean(fitted.values(mod))
```

Not surprisingly that `mean(observations)` equals `mean(fitted.values)`.    
If not, why are they called "fitted.values" to the model?   

<h4 id = 'mean(residuals)'></h4>
```{r}
# Mean of the residuals
mean(residuals(mod))
```
The least squares fitting procedure guarantees that the mean of the residuals is **zero** (numerical instability may result in the computed values not being exactly zero).   
```{r}
library(broom)
# Create bdims_tidy
bdims_tidy <- augment(mod)

# Glimpse the resulting data frame
glimpse(bdims_tidy)
```

<br>

<h3 id = '4.2'></h3>
## Use of Regression Model

<h4 id = '4.2.1'></h4>
#### **Prediction: Called out-of-sample**   

Once we have fit the model, we can now compute expected values for observations that were **not** present in the data on which the model was fit.   

* `predict(model, newdata = *)`

```{r}
ben <- data.frame('wgt' = 74.8, 'hgt' = 182.8)
predict(mod, newdata = ben)
```

According to this model, Ben should be 81 kg.   
However, he is only 74.8 kg.
So the residual here equals `74.8 - 81 = -6.2 `.    

<br>

<h4 id = '4.2.2'></h4>
#### **Adding Regression Line to Plot Manually**    

Here we're going to use `geom_abline()` again.    
Now it's time to combined what we've learned so far.    
```{r}
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_abline(intercept = coef(mod)[1], slope = coef(mod)[2],  color = "dodgerblue")
```

<br>

<h2 id = '5'></h2>
# **Model Fit**

<h3 id = '5.1'></h2>
## Assess Model Fit      

After setting a model fit for our data, can we assess the **quality** of it?    

One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are.      
<br>
<h4 id = '5.1.1'></h4>
#### **SSE: Sum of Squared Errors(Residuals)**    

<div style="width:600px; height:400px">    
![](D:\Desktop\1.png)
</div>

When trying to set up a linear model, what we actually do is finding the line has **the least** sums of squared deviation.     
<br>
For each **obsevation** which is represented by a point on a scatterplot,     
the **residual** is simply the **vertical** distance between that point and the line.   
<br>
In this example, we've highlighted residuals with gray arrows.    
If we can find a line makes the sums of `residuals ^ 2` smaller, it's a better model.   
<br>
Also we can just minimize the sum of `residuals`    
which is always zero since the positive one and the negative one often offset each other when adding up in the 'best fitted' model.     
In fact ,we've already discussed this before: [mean(residuals)](#mean(residuals))     
<br>
Conventionally, the sum of squared deviation are called **Sum of Squared Errors**.    
It can be easily computed after using **[`augment()`](#augment())** to  our model.     

> **`SSE = sum(.resid ^2) = (n()-1) * var(.resid)`**    

```{r}
augment(mod) %>%
  summarize(SSE = (n()-1) * var(.resid),
            SSE_also = sum(.resid ^2))
```
SSE is a single number that captures how much are model missed by.    
Unfortunately, it's hard to interpret since the units have been squared.    

<br>
<h4 id = '5.1.2'></h4>
#### **RMSE: Root Mean Squared Error**    

Besides SSE, another common measurement of the accuracy of a model is -- RMSE.    
RMSE is basicallay **the standard deviation of residuals**    

> **`RMSE = sqrt(sum(.resid ^ 2)/d.f) = sqrt(SEE/d.f)`**    

```{r}
summary(mod)
```

Using`summary(mod)` can easily get the value of RMSE which is named ** Residual Standard Error** in the summary.    
Conveniently, RMSE returns in the unit of reponse.    
In this case, `RMSE = 9.30804` which means:   
The typical difference between the observed `wgt` and the `wgt` predicted by the model is about `9.30804 'kg'`.     
<br>
And we can try to calculte RMSE manually using the formula above.   
```{r}
# Compute RMSE manually
sqrt(sum(residuals(mod)^2) / df.residual(mod))
```

<h3 id = '5.2'></h2>
## Compare Model Fit

<h4 id = '5.2.1'></h4>
#### **Null(Average) Model**    
If you have to predict without any regressed model, what would your prediction be?    
For most people, the answer is `mean` and that's **Null Model**. 

* For all observations.   
* `y-hat(Expected) = y-mean`    

<div style="width:600px; height:400px">    
![](D:\Desktop\2.png)
</div>

```{r}
# A Linear Model of bdims (same as before)
mod_smooth <- lm(wgt ~ hgt, data = bdims)
mod_smooth %>%
  augment() %>%
  summarise(SSE = sum(.resid ^ 2))

# A Null Model of bdims
mod_null <- lm(wgt ~ 1, data = bdims)
mod_null %>%
  augment() %>%
  summarise(SSE = sum(.resid ^ 2))
```
Since the SSE in `mod_smooth` is much smaller than the one in `mod_null`, `mod_smooth` is a better fit to `bdims`.    

<br>

<h4 id = '5.2.2'></h4>
#### **R^2: Coefficient of Determination**    

> **`R^2 = 1 - (SSE/SST) = 1 - var(e)/var(y)`**   

* R^2 is a measure of variability in a response variable.     
* SSE for a null model is often called **SST: Total Sums of Regress**   
* It's **a common measure of the fit of the regression model.**    
* For simple lunear model `r^2(x,y) = R^2`    
* A high/low R^2 doesn't mean taht the model is excellent/lousy.     
<br>

The R^2 is also stored in `summary(model)`
Let's have a look  again.   
```{r}
summary(mod)
```

R^2 is named 'R-squared' is the summary.    
In this case, we can interpret that about `51%` of the variability in `wgt` can be explained by `hgt`.  

<br>

<h3 id = '5.3'></h2>
## Unsual Points

<h4 id = '5.3.1'></h4>
#### **Leverage**    

> <div style="width:200px; height:50px">    
> ![](D:\Desktop\3.png)
> </div>

You don't need to understand the formula totally, but you should recognize that:    

* The leverage score for observation is about the distance between the value and the mean of explanatory data.     
    + Points are close to the horizontal center(mean of x) of the scatterplot have low leverages.
    + Points are far from the horizontal center(mean of x) of the scatterplot have high leverages.   
    + Nothing to do with y-coordinate.    

* Points of high leverage may or may not be influential. 
* Leverage can be retrieve by using [**`augment()`**](#augment()) and it's named by `.hat`.   

<br>

<h4 id = '5.3.2'></h4>
#### **Influence**
As noted previously, observations of high leverage may or may not be influential.   

* The influence of an observation depends on:   
    + Leverage: Takes into account the explanatory variable.          
    + Magnitude of its residual: depends on the response variable(`y`) and the fitted value(`y-hat`).     
    
* Influential points are likely to have **high** leverage and **deviate** from the general relationship between the two variables.    
* Influence can be retrieve by using [**`augment()`**](#augment()) and it's named by `.cooksd`.   

Let's take a look at both leverage & influence.   
```{r}
lm(formula = SLG ~ OBP, data = filter(mlbBat10, AB >= 10)) %>%
  augment() %>%
  arrange(desc(.hat), .cooksd) %>%
  select(SLG, OBP, .fitted, .hat, .cooksd) %>%
  head()
```

<br>

<h4 id = '5.3.3'></h4>
#### **Removing Outliers**

Observations can be outliers for a number of different reasons.     
Statisticians must always be careful—and more importantly, transparent—when dealing with outliers.    
Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model.     
However, one must have strong justification for doing this.     
A desire to have a higher R^2 is not a good enough reason!    
We're looking for the best model not the best fit model!    

Here's a simple example of removing 'Bobby Scales' from `mlbBat10`,   
but it's just an example since there is nothing to suggest that it is not a valid data point.   
```{r}
# Create nontrivial_players
nontrivial_players <- mlbBat10 %>%
  filter(AB >= 10 & OBP < 0.5)

# Fit model to new data
mod_cleaner <- lm(SLG ~ OBP, data = nontrivial_players)

# View model summary
summary(mod_cleaner)

# Visualize new model
ggplot(nontrivial_players, aes(x = OBP, y = SLG)) +
  geom_point() +
  geom_smooth(method = 'lm')
```